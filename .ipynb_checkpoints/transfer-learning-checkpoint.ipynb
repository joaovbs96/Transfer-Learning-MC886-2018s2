{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow version: 1.12.0-rc0\n",
    "scikit-learn version: 0.17\n",
    "keras version: 2.2.4\n",
    "tensorboard version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "In this assignment, we will use the weights of a network pre-trained in a particular problem as starting point to train our CNN to a different problem. As training a network from scratch is time-consuming and demands a lot of data, this is a frequent strategy, specially if both datasets (the one used for pre-training and the target) shares similar structures/elements/concepts. \n",
    "\n",
    "This is specially true when working with images. Most filters learned in initial convolutional layers will detect low-level elements, such as borders, corners and color blobs, which are common to most problems in the image domain. \n",
    "\n",
    "In this notebook, we will load the SqueezeNet architecture trained in the ImageNet dataset and fine-tune it to CIFAR-10.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Adamax, Nadam, SGD, Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# import preprocess input used in imagenet\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "#Utility to plot\n",
    "def plotImages(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "        \n",
    "        \n",
    "def plotImage(img):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.imshow(np.uint8(img), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet definition\n",
    "These methods define our architecture and load the weights obtained using ImageNet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The class are **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data. X:  (40000, 32, 32, 3) , Y:  (40000, 1)\n",
      "Validation data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n",
      "Test data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n",
      "\n",
      "Validation dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d873d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  truck\n",
      "\n",
      "test dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d86dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  ship\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
    "\n",
    "# separate data between train and validation\n",
    "valid_X, valid_Y = train_X[40000:], train_Y[40000:]\n",
    "train_X, train_Y = train_X[:40000], train_Y[:40000]\n",
    "\n",
    "print(\"Train data. X: \", train_X.shape, \", Y: \", train_Y.shape)\n",
    "print(\"Validation data. X: \", valid_X.shape, \", Y: \", valid_Y.shape)\n",
    "print(\"Test data. X: \", test_X.shape, \", Y: \", test_Y.shape)\n",
    "\n",
    "# The names of the classes in the cifar 10 dataset\n",
    "CLASS_NAMES = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "]\n",
    "\n",
    "# Show an example of an image and the label\n",
    "print(\"\\nValidation dataset\")\n",
    "plotImage(train_X[1])\n",
    "print(\"Result: \", CLASS_NAMES[np.asscalar(train_Y[1])])\n",
    "\n",
    "# Prepare the data\n",
    "# preprocess the image by subtracting the mean RGB pixel intensity from the ImageNet dataset\n",
    "# and show an example from the validation set\n",
    "#train_X = preprocess_input(train_X)\n",
    "#valid_X = preprocess_input(valid_X)\n",
    "#train_X = preprocess_input(train_X)\n",
    "\n",
    "print(\"\\ntest dataset\")\n",
    "plotImage(valid_X[1])\n",
    "print(\"Result: \", CLASS_NAMES[np.asscalar(valid_Y[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Transform targets to compatible format with cifar-10\n",
    "train_Y = np_utils.to_categorical(train_Y, 10)\n",
    "valid_Y = np_utils.to_categorical(valid_Y, 10)\n",
    "test_Y = np_utils.to_categorical(test_Y, 10)\n",
    "print(train_Y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## SqueezeNet with frozen layers\n",
    "Our initial attempt will be to remove SqueezeNet's top layers --- responsible for the classification into ImageNet classes --- and train a new set of layers to our CIFAR-10 classes. We will also freeze the layers before `drop9`. Our architecture will be like this:\n",
    "\n",
    "<img src=\"frozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_8', 'conv1', 'relu_conv1', 'pool1', 'fire2/squeeze1x1', 'fire2/relu_squeeze1x1', 'fire2/expand1x1', 'fire2/expand3x3', 'fire2/relu_expand1x1', 'fire2/relu_expand3x3', 'fire2/concat', 'fire3/squeeze1x1', 'fire3/relu_squeeze1x1', 'fire3/expand1x1', 'fire3/expand3x3', 'fire3/relu_expand1x1', 'fire3/relu_expand3x3', 'fire3/concat', 'pool3', 'fire4/squeeze1x1', 'fire4/relu_squeeze1x1', 'fire4/expand1x1', 'fire4/expand3x3', 'fire4/relu_expand1x1', 'fire4/relu_expand3x3', 'fire4/concat', 'fire5/squeeze1x1', 'fire5/relu_squeeze1x1', 'fire5/expand1x1', 'fire5/expand3x3', 'fire5/relu_expand1x1', 'fire5/relu_expand3x3', 'fire5/concat', 'pool5', 'fire6/squeeze1x1', 'fire6/relu_squeeze1x1', 'fire6/expand1x1', 'fire6/expand3x3', 'fire6/relu_expand1x1', 'fire6/relu_expand3x3', 'fire6/concat', 'fire7/squeeze1x1', 'fire7/relu_squeeze1x1', 'fire7/expand1x1', 'fire7/expand3x3', 'fire7/relu_expand1x1', 'fire7/relu_expand3x3', 'fire7/concat', 'fire8/squeeze1x1', 'fire8/relu_squeeze1x1', 'fire8/expand1x1', 'fire8/expand3x3', 'fire8/relu_expand1x1', 'fire8/relu_expand3x3', 'fire8/concat', 'fire9/squeeze1x1', 'fire9/relu_squeeze1x1', 'fire9/expand1x1', 'fire9/expand3x3', 'fire9/relu_expand1x1', 'fire9/relu_expand3x3', 'fire9/concat', 'drop9']\n"
     ]
    }
   ],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "#freeze layers\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# remove layers\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "\n",
    "print([layer.name for layer in squeezeNetModel.layers])\n",
    "\n",
    "#Add new classification layers\n",
    "#x = squeezeNetModel.output\n",
    "x = squeezeNetModel.layers[-1].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='conv10')(x)\n",
    "x = Activation('relu', name='relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='cifar10-frozen-layers')\n",
    "#print([layer.name for layer in model.layers])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 19s 463us/step - loss: 9.0491 - acc: 0.1007\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 8.0453 - acc: 0.1065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12331ff28>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model and train it.\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "    \n",
    "# train model\n",
    "model.fit(train_X, train_Y, epochs=epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 428us/step\n",
      "Validation loss: 4.457023458862305\n",
      "Validation accuracy (NORMALIZED): 0.1286\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation:\n",
    "score = model.evaluate(valid_X, valid_Y, verbose=1)\n",
    "\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "# Training last 2 Fire Modules + classification layers\n",
    "As we could see, the frozen network performed very poorly. By freezing most layers, we do not allow SqueezeNet to adapt its weights to features present in CIFAR-10.\n",
    "\n",
    "Let's try to unfreeze the last two fire modules and train once more. The architecture will be:\n",
    "<img src=\"partFrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_9', 'conv1', 'relu_conv1', 'pool1', 'fire2/squeeze1x1', 'fire2/relu_squeeze1x1', 'fire2/expand1x1', 'fire2/expand3x3', 'fire2/relu_expand1x1', 'fire2/relu_expand3x3', 'fire2/concat', 'fire3/squeeze1x1', 'fire3/relu_squeeze1x1', 'fire3/expand1x1', 'fire3/expand3x3', 'fire3/relu_expand1x1', 'fire3/relu_expand3x3', 'fire3/concat', 'pool3', 'fire4/squeeze1x1', 'fire4/relu_squeeze1x1', 'fire4/expand1x1', 'fire4/expand3x3', 'fire4/relu_expand1x1', 'fire4/relu_expand3x3', 'fire4/concat', 'fire5/squeeze1x1', 'fire5/relu_squeeze1x1', 'fire5/expand1x1', 'fire5/expand3x3', 'fire5/relu_expand1x1', 'fire5/relu_expand3x3', 'fire5/concat', 'pool5', 'fire6/squeeze1x1', 'fire6/relu_squeeze1x1', 'fire6/expand1x1', 'fire6/expand3x3', 'fire6/relu_expand1x1', 'fire6/relu_expand3x3', 'fire6/concat', 'fire7/squeeze1x1', 'fire7/relu_squeeze1x1', 'fire7/expand1x1', 'fire7/expand3x3', 'fire7/relu_expand1x1', 'fire7/relu_expand3x3', 'fire7/concat', 'fire8/squeeze1x1', 'fire8/relu_squeeze1x1', 'fire8/expand1x1', 'fire8/expand3x3', 'fire8/relu_expand1x1', 'fire8/relu_expand3x3', 'fire8/concat', 'fire9/squeeze1x1', 'fire9/relu_squeeze1x1', 'fire9/expand1x1', 'fire9/expand3x3', 'fire9/relu_expand1x1', 'fire9/relu_expand3x3', 'fire9/concat', 'drop9', 'conv10', 'relu_conv10', 'global_average_pooling2d_16', 'loss']\n",
      "input_9 False\n",
      "conv1 False\n",
      "relu_conv1 False\n",
      "pool1 False\n",
      "fire2/squeeze1x1 False\n",
      "fire2/relu_squeeze1x1 False\n",
      "fire2/expand1x1 False\n",
      "fire2/expand3x3 False\n",
      "fire2/relu_expand1x1 False\n",
      "fire2/relu_expand3x3 False\n",
      "fire2/concat False\n",
      "fire3/squeeze1x1 False\n",
      "fire3/relu_squeeze1x1 False\n",
      "fire3/expand1x1 False\n",
      "fire3/expand3x3 False\n",
      "fire3/relu_expand1x1 False\n",
      "fire3/relu_expand3x3 False\n",
      "fire3/concat False\n",
      "pool3 False\n",
      "fire4/squeeze1x1 False\n",
      "fire4/relu_squeeze1x1 False\n",
      "fire4/expand1x1 False\n",
      "fire4/expand3x3 False\n",
      "fire4/relu_expand1x1 False\n",
      "fire4/relu_expand3x3 False\n",
      "fire4/concat False\n",
      "fire5/squeeze1x1 False\n",
      "fire5/relu_squeeze1x1 False\n",
      "fire5/expand1x1 False\n",
      "fire5/expand3x3 False\n",
      "fire5/relu_expand1x1 False\n",
      "fire5/relu_expand3x3 False\n",
      "fire5/concat False\n",
      "pool5 False\n",
      "fire6/squeeze1x1 False\n",
      "fire6/relu_squeeze1x1 False\n",
      "fire6/expand1x1 False\n",
      "fire6/expand3x3 False\n",
      "fire6/relu_expand1x1 False\n",
      "fire6/relu_expand3x3 False\n",
      "fire6/concat False\n",
      "fire7/squeeze1x1 False\n",
      "fire7/relu_squeeze1x1 False\n",
      "fire7/expand1x1 False\n",
      "fire7/expand3x3 False\n",
      "fire7/relu_expand1x1 False\n",
      "fire7/relu_expand3x3 False\n",
      "fire7/concat False\n",
      "fire8/squeeze1x1 True\n",
      "fire8/relu_squeeze1x1 True\n",
      "fire8/expand1x1 True\n",
      "fire8/expand3x3 True\n",
      "fire8/relu_expand1x1 True\n",
      "fire8/relu_expand3x3 True\n",
      "fire8/concat True\n",
      "fire9/squeeze1x1 True\n",
      "fire9/relu_squeeze1x1 True\n",
      "fire9/expand1x1 True\n",
      "fire9/expand3x3 True\n",
      "fire9/relu_expand1x1 True\n",
      "fire9/relu_expand3x3 True\n",
      "fire9/concat True\n",
      "drop9 True\n",
      "conv10 True\n",
      "relu_conv10 True\n",
      "global_average_pooling2d_16 True\n",
      "loss True\n"
     ]
    }
   ],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "print([layer.name for layer in squeezeNetModel.layers])\n",
    "\n",
    "#freeze the mentioned layers\n",
    "for layer in squeezeNetModel.layers[:48]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in squeezeNetModel.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# remove layers\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "\n",
    "#Add new classification layers\n",
    "x = squeezeNetModel.layers[-1].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='conv10')(x)\n",
    "x = Activation('relu', name='relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(squeezeNetModel.inputs, x, name='cifar10-train-last-two-fire-layers')\n",
    "#print([layer.name for layer in model.layers])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 27s 676us/step - loss: 3.8795 - acc: 0.1128\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 25s 635us/step - loss: 2.3148 - acc: 0.1263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132261400>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile model and train it\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "    \n",
    "# train model\n",
    "model.fit(train_X, train_Y, epochs=epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 418us/step\n",
      "Validation loss: 2.235505826950073\n",
      "Validation accuracy (NORMALIZED): 0.1689\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation.\n",
    "score = model.evaluate(test_X, test_Y, verbose=1)\n",
    "\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a visualization tool for Tensorflow. Among other things, it allows us to monitor the progress of our training, plot metrics per epochs, visualize the architecture's schematics. \n",
    "\n",
    "Just like for Early Stopping, we will use the [Tensorboard callback](https://keras.io/callbacks/#tensorboard) to log the information about our training. An example of usage, would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Just an example, DON'T RUN! \n",
    "### You will need to change <<LOG_DIR>>\n",
    "import keras.callbacks as callbacks\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./<<LOG_DIR>>\")\n",
    "model.fit(..., callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your training progresses, Keras will log the metrics (e.g., loss, accuracy) to `<<LOG_DIR>>` (**make sure `<<LOG_DIR>>` is a valid directory)**. On your terminal, you will need to run Tensorboard, assign a port and access it via browser (just like jupyter).\n",
    "\n",
    "#### ----> MAKE SURE YOU USE A DIFFERENT PORT FOR JUPYTER AND TENSORBOARD <----\n",
    "\n",
    "### Docker\n",
    "For those using docker, open a new terminal and create a new container (using the same image) running Tensorboard:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p <<port_host>>:<<port_container>>\n",
    "            --volume=<<LOG_DIR>>:<<LOG_DIR>>\n",
    "            --name=<<container_name>> <<docker_image>> \n",
    "            tensorboard --logdir=<<LOG_DIR>> --port=<<port_container>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p 8887:8887\n",
    "            --volume=/your/path/ml2018/:/ml2018\n",
    "            --name=mdc_container_tensorboard mdc-keras:cpu\n",
    "            tensorboard --logdir=/ml2018/logs --port=8887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port_container>>`.\n",
    "\n",
    "### Anaconda\n",
    "$ tensorboard --logdir=<<LOG_DIR>> --port=<<port>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port>>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "# Fine-tuning all layers\n",
    "\n",
    "What if we fine-tune all layers of SqueezeNet?\n",
    "<img src=\"unfrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "# Remove layers\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "squeezeNetModel.layers.pop()\n",
    "\n",
    "# Add new classification layers\n",
    "x = squeezeNetModel.layers[-1].output\n",
    "x = Convolution2D(10, (1, 1), padding='valid', name='conv10')(x)\n",
    "x = Activation('relu', name='relu_conv10')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "# Create a new model\n",
    "model = Model(squeezeNetModel.inputs, x, name='cifar10-fine-tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 2.1507 - acc: 0.2397\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.5326 - acc: 0.4806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12da96080>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "#Compile model\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Tensorboard callback\n",
    "tbCallBack = TensorBoard(log_dir=\"./tensorboard\".format(time()), write_graph=True)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#Fine-tuning the model\n",
    "model.fit(train_X, train_Y, epochs=epochs, shuffle=True, verbose=1, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 437us/step\n",
      "Validation loss: 1.1968740039825438\n",
      "Validation accuracy (NORMALIZED): 0.5899\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation.\n",
    "score = model.evaluate(valid_X, valid_Y, verbose=1)\n",
    "\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 362us/step\n",
      "Validation loss: 1.19598601398468\n",
      "Validation accuracy (NORMALIZED): 0.5912\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your best model on test\n",
    "score = model.evaluate(test_X, test_Y, verbose=1)\n",
    "\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we are working on more complex tasks and our trainings are starting to take more time it is usually a good idea to save the trained model from time to time. [Keras has a lot of ways of saving and loading the model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model), but in this exercise we will use the simplest of them all: `model.save()`. It saves the architecture, the weights, the choice of loss function/optimizer/metrics and even the current state of the training, so you can resume your training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "Once we have our model trained, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 545us/step\n",
      "Test loss: 1.19598601398468\n",
      "Test accuracy (NORMALIZED): 0.5912\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "del model  # Will delete model, only to check if load_model is working\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# evaluate test set again... should give us the same result\n",
    "score = model.evaluate(test_X, test_Y, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
